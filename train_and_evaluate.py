# filename: train_and_evaluate.py (å…³é”®æ›´æ–°éƒ¨åˆ†)

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, AdamW # å¯¼å…¥ AdamW ä¼˜åŒ–å™¨
import numpy as np

# å¯¼å…¥æ‰€æœ‰å¸¸é‡å’Œæ¨¡å‹
from constants import TAG_TO_ID, SPAN_LABEL_TO_ID, ENTITY_TYPES
from models import LayeringNERModel, SingleTypeNERModel, SpanBasedNERModel, ReasoningIENERModel, MODEL_NAME

# --- è¾…åŠ©å‡½æ•°ï¼šå°†æ ‡ç­¾å­—ç¬¦ä¸²è½¬æ¢ä¸º ID ---
def tags_to_ids(tags, tag_map):
    # å°†åˆ—è¡¨ä¸­çš„æ ‡ç­¾å­—ç¬¦ä¸²æ˜ å°„ä¸º IDï¼Œå¯¹äºä¸å­˜åœ¨çš„æ ‡ç­¾ ID ä½¿ç”¨ -100 (ç”¨äº CrossEntropyLoss å¿½ç•¥)
    return [tag_map.get(t, -100) for t in tags]

# --- é€šç”¨ Data Collator (é’ˆå¯¹åºåˆ—æ ‡æ³¨) ---
def data_collator_sequence(batch, tokenizer, tag_map, is_layering=False):
    tokens = [item['tokens'] for item in batch]
    
    # 1. Tokenization (ä½¿ç”¨ BERT tokenizer)
    encoded_inputs = tokenizer(tokens, 
                               is_split_into_words=True, 
                               padding='max_length', 
                               truncation=True, 
                               return_tensors='pt')
    input_ids = encoded_inputs['input_ids']
    attention_mask = encoded_inputs['attention_mask']
    
    batch_labels_outer = []
    batch_labels_inner = []
    
    for i, item in enumerate(batch):
        # 2. æ ‡ç­¾å¯¹é½åˆ° Sub-word çº§åˆ«
        word_ids = encoded_inputs.word_ids(batch_index=i)
        
        # åˆå§‹æ ‡ç­¾åºåˆ—ï¼ˆLayering æˆ– Cascadingï¼‰
        if is_layering:
            raw_tags_outer = item['outer_tags']
            raw_tags_inner = item['inner_tags']
        else:
            # å‡è®¾ Cascading æ¨¡å‹æ‰¹æ¬¡å¤„ç†çš„æ˜¯æŸä¸ªç‰¹å®šå®ä½“çš„æ ‡ç­¾
            # ç®€åŒ–ï¼šè¿™é‡Œéœ€è¦ç”¨æˆ·ä¼ å…¥è¦å¤„ç†çš„å®ä½“ç±»å‹
            etype = 'PER' # å‡è®¾å½“å‰æ‰¹æ¬¡ç”¨äºè®­ç»ƒ PER æ¨¡å‹
            raw_tags_outer = item[f'{etype}_tags']
            raw_tags_inner = [t.replace(f'B-{etype}', 'B').replace(f'I-{etype}', 'I') 
                              for t in raw_tags_outer] # Cascading ä»…éœ€ B/I/O
            
        # 3. æ ‡ç­¾å¯¹é½ (åªå¯¹ç¬¬ä¸€ä¸ª Sub-word èµ‹äºˆæ ‡ç­¾ï¼Œå…¶ä»– Sub-word ä½¿ç”¨ -100 å¿½ç•¥)
        previous_word_idx = None
        label_ids_outer = []
        label_ids_inner = []
        
        for word_idx in word_ids:
            if word_idx is None:
                # [CLS], [SEP], Padding token 
                label_ids_outer.append(-100)
                label_ids_inner.append(-100)
            elif word_idx != previous_word_idx:
                # å¥å­ä¸­çš„ç¬¬ä¸€ä¸ª sub-word æ ‡è®°å®é™…æ ‡ç­¾
                tag_outer = raw_tags_outer[word_idx]
                label_ids_outer.append(tag_map.get(tag_outer, -100))
                
                tag_inner = raw_tags_inner[word_idx]
                # Cascading/Single-Type æ¨¡å‹éœ€è¦ B/I/O æ˜ å°„ (ç®€åŒ–ä¸º 0, 1, 2)
                tag_map_single = {'O': 0, f'B-{etype}': 1, f'I-{etype}': 2}
                label_ids_inner.append(tag_map_single.get(tag_inner, -100) if not is_layering else tag_map.get(tag_inner, -100))

            else:
                # åŒä¸€ä¸ªè¯çš„åç»­ sub-word å¿½ç•¥æŸå¤±
                label_ids_outer.append(-100)
                label_ids_inner.append(-100)

            previous_word_idx = word_idx

        batch_labels_outer.append(label_ids_outer)
        batch_labels_inner.append(label_ids_inner)

    if is_layering:
        return encoded_inputs['input_ids'], attention_mask, torch.tensor(batch_labels_outer), torch.tensor(batch_labels_inner)
    else:
        # Cascading åªè¿”å›ä¸€ä¸ªæ ‡ç­¾é›† (ç®€åŒ–)
        return encoded_inputs['input_ids'], attention_mask, torch.tensor(batch_labels_inner) 
    
# --- Span-Based Data Collator ---
def data_collator_span_based(batch, tokenizer, span_label_map, max_spans=100):
    
    # ç®€åŒ–ï¼šä»…å¤„ç† batch_size=1 çš„æƒ…å†µï¼ŒSpan-Based æ¨¡å‹çš„æ‰¹å¤„ç†å®ç°éå¸¸å¤æ‚
    if len(batch) > 1:
        # åœ¨å®é™…å®éªŒä¸­ï¼Œä½ éœ€è¦å®ç°å¤æ‚çš„åŠ¨æ€ Padding å’Œ Span ç»Ÿä¸€åŒ–
        raise ValueError("Span-Based æ¨¡å‹çš„ Data Collator ä»…æ”¯æŒ batch_size=1 æ¼”ç¤ºã€‚")
        
    item = batch[0]
    tokens = item['tokens']
    
    encoded_inputs = tokenizer(tokens, is_split_into_words=True, padding='max_length', truncation=True, return_tensors='pt')
    input_ids = encoded_inputs['input_ids']
    attention_mask = encoded_inputs['attention_mask']
    
    # æå– Span æ•°æ®
    all_spans = item['spans']
    
    # ä»…ä½¿ç”¨å‰ max_spans ä¸ª spanï¼Œç¡®ä¿æ¼”ç¤ºç®€æ´
    selected_spans = all_spans[:max_spans] 
    
    start_indices = [s['span'][0] + 1 for s in selected_spans] # +1 åç§»é‡ç”¨äº BERT çš„ [CLS]
    end_indices = [s['span'][1] for s in selected_spans]
    
    span_widths = [s['span'][1] - s['span'][0] for s in selected_spans]
    span_labels_ids = [span_label_map.get(s['label'], 0) for s in selected_spans]
    
    # å°†åˆ—è¡¨è½¬æ¢ä¸º Tensor
    start_indices = torch.tensor(start_indices, dtype=torch.long)
    end_indices = torch.tensor(end_indices, dtype=torch.long)
    span_widths = torch.tensor(span_widths, dtype=torch.long)
    span_labels_ids = torch.tensor(span_labels_ids, dtype=torch.long)
    
    return input_ids, attention_mask, start_indices, end_indices, span_widths, span_labels_ids

# --- è®­ç»ƒä¸è¯„ä¼°é€»è¾‘ ---

def train_and_evaluate(method_name, model, dataset, data_collator_fn, tag_map, num_epochs=3, learning_rate=1e-5):
    print(f"\n--- å¯åŠ¨ {method_name} æ–¹æ³•è®­ç»ƒ ---")
    
    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
    optimizer = AdamW(model.parameters(), lr=learning_rate)
    
    # æ ¹æ®æ¨¡å‹æ–¹æ³•é€‰æ‹© Collator
    if "Layering" in method_name:
        collator = lambda batch: data_collator_sequence(batch, tokenizer, tag_map, is_layering=True)
        batch_size = 4
    elif "Cascading" in method_name:
        collator = lambda batch: data_collator_sequence(batch, tokenizer, tag_map, is_layering=False)
        batch_size = 4
    elif "Span-Based" in method_name or "ReasoningIE" in method_name:
        collator = lambda batch: data_collator_span_based(batch, tokenizer, tag_map)
        # Span-Based æ¼”ç¤ºåªèƒ½ç”¨ batch_size=1
        batch_size = 1 
    else:
        raise ValueError("Unknown method name")
    
    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collator, shuffle=True)
    
    # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        
        # ä½¿ç”¨è¿›åº¦æ¡ï¼ˆçœç•¥å®é™…å®ç°ï¼‰
        print(f"Epoch {epoch+1}/{num_epochs}")
        for i, batch in enumerate(dataloader):
            optimizer.zero_grad()
            
            if "Layering" in method_name:
                input_ids, attention_mask, labels_outer, labels_inner = batch
                loss, _, _ = model(input_ids, attention_mask, labels_outer, labels_inner)
            
            elif "Cascading" in method_name:
                input_ids, attention_mask, labels = batch # labels æ˜¯å•ä¸ªå®ä½“çš„æ ‡ç­¾
                loss, _ = model(input_ids, attention_mask, labels)
                
            elif "Span-Based" in method_name or "ReasoningIE" in method_name:
                # Span-Based/ReasoningIE çš„ forward æ¥æ”¶ 6 ä¸ªå‚æ•°
                input_ids, attention_mask, start_indices, end_indices, span_widths, span_labels = batch
                loss, _ = model(input_ids, attention_mask, start_indices, end_indices, span_widths, span_labels)
                
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            if (i + 1) % 10 == 0:
                 print(f"  Batch {i+1}/{len(dataloader)} Loss: {total_loss / (i+1):.4f}")

    print(f"è®­ç»ƒå®Œæˆï¼š{method_name}ã€‚")
    # æ¨¡æ‹Ÿè¯„ä¼°ç»“æœ
    # ... (è¯„ä¼°é€»è¾‘çœç•¥)
    # print(f"*** æ¨¡æ‹Ÿè¯„ä¼°ç»“æœï¼š F1 Score: {0.75 + torch.randn(1).item() * 0.05:.4f} ***")

# --- ä¸»å®éªŒå‡½æ•° (éœ€è¦ä¿®æ”¹ä»¥è°ƒç”¨æ–°çš„ train_and_evaluate) ---

def run_experiment_pipeline(data_files):
    # 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† (å‡è®¾ data_processor.py å·²å®ç° load_data å’Œ preprocess_for_all_methods)
    # raw_data = load_data(data_files[0]) 
    # processed_data = preprocess_for_all_methods(raw_data)
    
    # ğŸš¨ ç”±äºæ— æ³•è®¿é—®åŸå§‹æ–‡ä»¶ï¼Œè¿™é‡Œéœ€è¦æ¨¡æ‹Ÿ processed_data ä»¥ä¾¿æ¼”ç¤ºæ¨¡å‹åˆå§‹åŒ–
    class DummyDataset(Dataset):
        def __init__(self, size):
            self.data = [{'tokens': ['è¿™', 'æ˜¯', 'ä¸€', 'ä¸ª', 'ç¤º', 'ä¾‹'], 
                          'outer_tags': ['B-ORG', 'I-ORG', 'O', 'B-PER', 'I-PER', 'O'],
                          'inner_tags': ['B-PER', 'I-PER', 'O', 'O', 'O', 'O'],
                          'PER_tags': ['O', 'O', 'O', 'B-PER', 'I-PER', 'O'], # Cascading
                          'spans': [{'span': (0, 2), 'label': 'ORG'}, {'span': (3, 5), 'label': 'PER'}]
                          } for _ in range(size)]
        def __len__(self): return len(self.data)
        def __getitem__(self, idx): return self.data[idx]
        
    print("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¨¡å‹åˆå§‹åŒ–å’Œè®­ç»ƒé€»è¾‘æ¼”ç¤º...")
    dummy_size = 50
    
    # 2. æ¨¡å‹å®šä¹‰å’Œå®éªŒè¿è¡Œ
    
    # --- æ–¹æ³• 1: Layering Method ---
    layering_model = LayeringNERModel()
    layering_dataset = DummyDataset(dummy_size)
    # train_and_evaluate("Layering Method (Outer/Inner BIO)", layering_model, layering_dataset, data_collator_sequence, TAG_TO_ID)
    
    # --- æ–¹æ³• 2: Cascading Method (ä»¥ PER å®ä½“ä¸ºä¾‹) ---
    # Cascading æ¨¡å‹çš„æ ‡ç­¾æ˜ å°„æ˜¯ç²¾ç®€çš„ (O/B-PER/I-PER)
    cascading_per_model = SingleTypeNERModel('PER')
    cascading_dataset = DummyDataset(dummy_size)
    # train_and_evaluate("Cascading Method (PER Entity)", cascading_per_model, cascading_dataset, data_collator_sequence, {'O':0, 'B-PER':1, 'I-PER':2})

    # --- æ–¹æ³• 3: Span-Based Method ---
    span_model = SpanBasedNERModel()
    span_dataset = DummyDataset(dummy_size)
    # train_and_evaluate("Enumeration/Span-Based Method", span_model, span_dataset, data_collator_span_based, SPAN_LABEL_TO_ID)

    # --- æ–¹æ³• 4: ReasoningIE Style ---
    reasoning_model = ReasoningIENERModel()
    reasoning_dataset = DummyDataset(dummy_size)
    
    print("\n------------------------------------------------------------")
    print("æ¨¡å‹å¡«å……å®Œæˆã€‚ä»¥ä¸‹æ˜¯è¿è¡Œ Layering Method çš„æ¨¡æ‹Ÿè®­ç»ƒæ¼”ç¤ºï¼š")
    print("------------------------------------------------------------")
    
    # å®é™…è¿è¡Œ Layering Method æ¼”ç¤º
    train_and_evaluate("Layering Method (Outer/Inner BIO)", layering_model, layering_dataset, None, TAG_TO_ID)


if __name__ == '__main__':  
    run_experiment_pipeline(["dummy_file.jsonlines"]) # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿è¡Œ